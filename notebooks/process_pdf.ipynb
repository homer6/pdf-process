{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://medium.com/geekculture/automating-pdf-interaction-with-langchain-and-chatgpt-e723337f26a6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o paper.pdf https://arxiv.org/pdf/2303.13519.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain pypdf chromadb openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader # for loading the pdf\n",
    "from langchain.embeddings import OpenAIEmbeddings # for creating embeddings\n",
    "from langchain.vectorstores import Chroma # for the vectorization part\n",
    "from langchain.chains import ChatVectorDBChain # for chatting with the pdf\n",
    "from langchain.llms import OpenAI # the LLM model we'll use (CHatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./paper.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load_and_split()\n",
    "# print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "api_key = 'xxxx'\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(pages, embedding=embeddings, persist_directory=\".\")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_qa = ChatVectorDBChain.from_llm(OpenAI(temperature=0.5, model_name=\"gpt-3.5-turbo\"), vectordb, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "VideoTaskformer is a pre-trained video model that focuses on representing the semantics and structure of instructional videos. It uses a simple and effective objective of predicting weakly supervised textual labels for steps that are randomly masked out from an instructional video (masked step modeling). It learns step embeddings that encapsulate the semantics and temporal ordering of steps within a task, making them context-aware and possessing global knowledge of task structure. It can verify if an unseen video correctly executes a given task, as well as forecast which steps are likely to be taken after a given step. It outperforms previous baselines on various benchmark tasks and achieves new state-of-the-art performance.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the VideoTaskformer?\"\n",
    "result = pdf_qa({\"question\": query, \"chat_history\": \"\"})\n",
    "print(\"Answer:\")\n",
    "print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
